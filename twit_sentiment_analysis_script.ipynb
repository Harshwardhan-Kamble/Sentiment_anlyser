{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f34bfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow pandas numpy scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea0b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# NLP imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Download required NLTK data\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88fc7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set GPU memory growth to avoid OOM errors\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cff712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== DATA PREPROCESSING ==============\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_tweet(self, text):\n",
    "        \"\"\"Clean and preprocess tweet text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove user mentions but keep hashtags as they might be meaningful\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        # Remove hashtag symbol but keep the word\n",
    "        text = re.sub(r'#', '', text)\n",
    "        \n",
    "        # Remove special characters and digits but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # For gaming tweets, we might want to keep some gaming-specific terms\n",
    "        # Don't remove all stopwords for better context in gaming tweets\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Only remove very common stopwords that don't affect sentiment\n",
    "        very_common_stops = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}\n",
    "        tokens = [word for word in tokens if word not in very_common_stops or len(word) > 2]\n",
    "        \n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020add98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 61692 tweets\n",
      "Entities: entity\n",
      "TomClancysGhostRecon    2322\n",
      "MaddenNFL               2310\n",
      "TomClancysRainbowSix    2304\n",
      "Microsoft               2226\n",
      "Nvidia                  2208\n",
      "Name: count, dtype: int64\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "negative    22542\n",
      "positive    20832\n",
      "neutral     18318\n",
      "Name: count, dtype: int64\n",
      "Dataset shape: (61692, 3)\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "0    22542\n",
      "2    20832\n",
      "1    18318\n",
      "Name: count, dtype: int64\n",
      "Sample tweets:\n",
      "                                                text  sentiment\n",
      "0  im getting on borderlands and i will murder yo...          2\n",
      "1  I am coming to the borders and I will kill you...          2\n",
      "2  im getting on borderlands and i will kill you ...          2\n",
      "3  im coming on borderlands and i will murder you...          2\n",
      "4  im getting on borderlands 2 and i will murder ...          2\n"
     ]
    }
   ],
   "source": [
    "# ============== LOAD AND PREPARE DATA ==============\n",
    "\n",
    "def load_twitter_data(filepath='twitter_training.csv'):\n",
    "    \"\"\"\n",
    "    Load Twitter dataset with format: ID, Entity, Sentiment, Text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load CSV with proper column names\n",
    "        # Assuming columns are: ID, Entity, Sentiment, Text\n",
    "        df = pd.read_csv(filepath, names=['id', 'entity', 'sentiment', 'text'], \n",
    "                         header=None, encoding='utf-8')\n",
    "        \n",
    "        # Convert sentiment labels to lowercase for consistency\n",
    "        df['sentiment'] = df['sentiment'].str.lower().str.strip()\n",
    "        \n",
    "        # Map sentiment to numerical values\n",
    "        sentiment_mapping = {\n",
    "            'positive': 2,\n",
    "            'neutral': 1,\n",
    "            'negative': 0\n",
    "        }\n",
    "        \n",
    "        # Apply mapping\n",
    "        df['sentiment_numeric'] = df['sentiment'].map(sentiment_mapping)\n",
    "        \n",
    "        # Remove rows with unmapped sentiments\n",
    "        df = df.dropna(subset=['sentiment_numeric'])\n",
    "        df['sentiment_numeric'] = df['sentiment_numeric'].astype(int)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} tweets\")\n",
    "        print(f\"Entities: {df['entity'].value_counts().head()}\")\n",
    "        print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
    "        \n",
    "        # Rename for consistency with rest of code\n",
    "        df['sentiment'] = df['sentiment_numeric']\n",
    "        \n",
    "        return df[['text', 'sentiment', 'entity']]\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filepath} not found. Using sample data for demonstration.\")\n",
    "        return load_sample_data()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Using sample data for demonstration.\")\n",
    "        return load_sample_data()\n",
    "\n",
    "def load_sample_data():\n",
    "    \"\"\"\n",
    "    Sample data for demonstration if file not found\n",
    "    \"\"\"\n",
    "    # Sample gaming-related tweets similar to your format\n",
    "    sample_tweets = [\n",
    "        \"im getting on borderlands and i will murder you all\",\n",
    "        \"This game is terrible, waste of money\",\n",
    "        \"Just another average shooter game\",\n",
    "        \"Best game ever! Can't stop playing\",\n",
    "        \"Worst gaming experience, uninstalling now\",\n",
    "        \"Gameplay is okay but nothing special\",\n",
    "        \"Love this game so much, amazing graphics\",\n",
    "        \"Broken mechanics, needs major fixes\",\n",
    "        \"It's alright, worth trying on sale\",\n",
    "        \"Absolutely fantastic multiplayer experience\"\n",
    "    ] * 50  # Multiply for more training data\n",
    "    \n",
    "    sample_entities = ['Borderlands', 'Gaming', 'Borderlands', 'Gaming', 'FPS',\n",
    "                      'Borderlands', 'Gaming', 'FPS', 'Borderlands', 'Gaming'] * 50\n",
    "    \n",
    "    # Labels: 0=negative, 1=neutral, 2=positive\n",
    "    sample_labels = [2, 0, 1, 2, 0, 1, 2, 0, 1, 2] * 50\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': sample_tweets,\n",
    "        'sentiment': sample_labels,\n",
    "        'entity': sample_entities\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load your data\n",
    "# Try to load the actual CSV file, fallback to sample if not found\n",
    "df = load_twitter_data('twitter_training.csv')  # Specify your file path here\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
    "print(f\"Sample tweets:\")\n",
    "print(df[['text', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86a808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after cleaning: (60590, 4)\n"
     ]
    }
   ],
   "source": [
    "# ============== PREPROCESS DATA ==============\n",
    "\n",
    "preprocessor = TweetPreprocessor()\n",
    "df['cleaned_text'] = df['text'].apply(preprocessor.clean_tweet)\n",
    "\n",
    "# Remove empty texts\n",
    "df = df[df['cleaned_text'] != '']\n",
    "\n",
    "print(f\"Dataset after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b5012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (48472, 100)\n",
      "Test set: (12118, 100)\n"
     ]
    }
   ],
   "source": [
    "# ============== TOKENIZATION AND PADDING ==============\n",
    "\n",
    "# Hyperparameters optimized for 8GB GPU\n",
    "MAX_WORDS = 10000  # Vocabulary size\n",
    "MAX_LEN = 100      # Maximum sequence length\n",
    "EMBEDDING_DIM = 128  # Embedding dimension\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "X = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Prepare labels\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Convert to categorical (one-hot encoding)\n",
    "num_classes = len(np.unique(y))\n",
    "y_categorical = keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03a4859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== BUILD MODEL ==============\n",
    "\n",
    "def create_lstm_model():\n",
    "    \"\"\"Create LSTM model optimized for 8GB GPU\"\"\"\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "        \n",
    "        # Bidirectional LSTM with dropout\n",
    "        Bidirectional(LSTM(64, return_sequences=True, dropout=0.2)),\n",
    "        Bidirectional(LSTM(32, dropout=0.2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_cnn_model():\n",
    "    \"\"\"Alternative: CNN model (faster training, less GPU memory)\"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Choose model based on your preference\n",
    "# Use CNN for faster training and less memory usage\n",
    "# Use LSTM for potentially better accuracy\n",
    "model = create_lstm_model()  # or create_cnn_model()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "867c3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.5306 - loss: 0.9440\n",
      "Epoch 1: val_accuracy improved from None to 0.77318, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 119ms/step - accuracy: 0.6480 - loss: 0.8043 - val_accuracy: 0.7732 - val_loss: 0.5777 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8237 - loss: 0.4852\n",
      "Epoch 2: val_accuracy improved from 0.77318 to 0.82341, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 112ms/step - accuracy: 0.8313 - loss: 0.4643 - val_accuracy: 0.8234 - val_loss: 0.4538 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8841 - loss: 0.3210\n",
      "Epoch 3: val_accuracy improved from 0.82341 to 0.84332, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 117ms/step - accuracy: 0.8838 - loss: 0.3232 - val_accuracy: 0.8433 - val_loss: 0.4181 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9101 - loss: 0.2524\n",
      "Epoch 4: val_accuracy improved from 0.84332 to 0.85673, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 115ms/step - accuracy: 0.9069 - loss: 0.2548 - val_accuracy: 0.8567 - val_loss: 0.4195 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9279 - loss: 0.1958\n",
      "Epoch 5: val_accuracy improved from 0.85673 to 0.86343, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 109ms/step - accuracy: 0.9237 - loss: 0.2066 - val_accuracy: 0.8634 - val_loss: 0.4882 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9388 - loss: 0.1662\n",
      "Epoch 6: val_accuracy improved from 0.86343 to 0.86931, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 109ms/step - accuracy: 0.9348 - loss: 0.1747 - val_accuracy: 0.8693 - val_loss: 0.4156 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9458 - loss: 0.1429\n",
      "Epoch 7: val_accuracy improved from 0.86931 to 0.87024, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 121ms/step - accuracy: 0.9444 - loss: 0.1471 - val_accuracy: 0.8702 - val_loss: 0.4864 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9488 - loss: 0.1317\n",
      "Epoch 8: val_accuracy improved from 0.87024 to 0.87695, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 124ms/step - accuracy: 0.9492 - loss: 0.1343 - val_accuracy: 0.8769 - val_loss: 0.4756 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9558 - loss: 0.1123\n",
      "Epoch 9: val_accuracy did not improve from 0.87695\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 122ms/step - accuracy: 0.9543 - loss: 0.1174 - val_accuracy: 0.8676 - val_loss: 0.5861 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9639 - loss: 0.0897\n",
      "Epoch 10: val_accuracy improved from 0.87695 to 0.88179, saving model to best_model.keras\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 122ms/step - accuracy: 0.9627 - loss: 0.0916 - val_accuracy: 0.8818 - val_loss: 0.5664 - learning_rate: 5.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9666 - loss: 0.0748\n",
      "Epoch 11: val_accuracy did not improve from 0.88179\n",
      "\u001b[1m1212/1212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 121ms/step - accuracy: 0.9669 - loss: 0.0792 - val_accuracy: 0.8814 - val_loss: 0.6483 - learning_rate: 5.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    }
   ],
   "source": [
    "# ============== TRAIN MODEL ==============\n",
    "\n",
    "# Callbacks for better training\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model with smaller batch size for GPU memory optimization\n",
    "BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d11bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.8841\n",
      "Test Loss: 0.5470\n",
      "\u001b[1m379/379\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.90      0.90      4436\n",
      "     Neutral       0.87      0.86      0.87      3582\n",
      "    Positive       0.88      0.88      0.88      4100\n",
      "\n",
      "    accuracy                           0.88     12118\n",
      "   macro avg       0.88      0.88      0.88     12118\n",
      "weighted avg       0.88      0.88      0.88     12118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============== EVALUATE MODEL ==============\n",
    "\n",
    "# Load best model\n",
    "model = load_model('best_model.keras')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=sentiment_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1591f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved as 'twitter_sentiment_model.keras'\n",
      "✓ Tokenizer saved as 'twitter_sentiment_tokenizer.pickle'\n",
      "✓ Configuration saved as 'twitter_sentiment_config.pickle'\n",
      "\n",
      "✅ All model artifacts saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============== SAVE MODEL AND PREPROCESSORS ==============\n",
    "\n",
    "def save_model_artifacts(model, tokenizer, preprocessor, model_name='twitter_sentiment'):\n",
    "    \"\"\"Save all model artifacts for future use\"\"\"\n",
    "    \n",
    "    # 1. Save the Keras model\n",
    "    model.save(f'{model_name}_model.keras')\n",
    "    print(f\"✓ Model saved as '{model_name}_model.keras'\")\n",
    "    \n",
    "    # 2. Save the tokenizer\n",
    "    with open(f'{model_name}_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"✓ Tokenizer saved as '{model_name}_tokenizer.pickle'\")\n",
    "    \n",
    "    # 3. Save preprocessing configuration\n",
    "    config = {\n",
    "        'max_words': MAX_WORDS,\n",
    "        'max_len': MAX_LEN,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'num_classes': num_classes,\n",
    "        'sentiment_labels': sentiment_labels\n",
    "    }\n",
    "    with open(f'{model_name}_config.pickle', 'wb') as handle:\n",
    "        pickle.dump(config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"✓ Configuration saved as '{model_name}_config.pickle'\")\n",
    "    \n",
    "    print(\"\\n✅ All model artifacts saved successfully!\")\n",
    "\n",
    "# Save everything\n",
    "save_model_artifacts(model, tokenizer, preprocessor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
